{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "The objective of this assignment is to get you familiarizewith  the  problems  of  `classification`  and  `verification`with a popular problem space of `face`\n",
    "\n",
    "This jupyter notebook is meant to be used in conjunction with the full questions in the assignment pdf.\n",
    "\n",
    "## Instructions\n",
    "- Write your code and analyses in the indicated cells.\n",
    "- Ensure that this notebook runs without errors when the cells are run in sequence.\n",
    "- Do not attempt to change the contents of the other cells.\n",
    "\n",
    "## Allowed Libraries\n",
    "- All libraries are allowed \n",
    "\n",
    "## Datasets \n",
    "- 3 datasets are provided. Load the data from the drive [link](!https://drive.google.com/file/d/1ujsKv9W5eidb4TXt1pnsqwDKVDFtzZTh/view?usp=sharing).\n",
    "- Unzip the downloaded file and store the files in a folder called `datasets`. Keep the `datasets` folder in the same directory as of the jupyter notebook \n",
    "\n",
    "## Submission\n",
    "- Ensure that this notebook runs without errors when the cells are run in sequence.\n",
    "- Rename the notebook to `<roll_number>.ipynb` and submit ONLY the notebook file on moodle.\n",
    "- Upload  the  notebook,  report  and  classification  results as a zip file to moodle. Name the zip file as `<rollnumber>_assignment2.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Imports\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import numpy as  np\n",
    "import pandas as pd\n",
    "from scipy import linalg\n",
    "\n",
    "# Loading and plotting data\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "# Features\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.discriminant_analysis import _class_means,_class_cov\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.manifold import TSNE,Isomap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Define your classifier here. You can use libraries like sklearn to create your classifier \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix,precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "plt.ion()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "- Image size: Bigger images create better representation but would require more computation. Choose the correct image size based on your Laptop configuration. \n",
    "- is_grayscale: Should you take grayscale images? Or rgb images? Choose whichever gives better representation for classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = {\n",
    "    'image_size': 64,\n",
    "    'is_grayscale': False,\n",
    "    'val_split': 0.75\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfw_dict = {'Amitabhbachan': 0,\n",
    "    'AamirKhan': 1,\n",
    "    'DwayneJohnson': 2,\n",
    "    'AishwaryaRai': 3,\n",
    "    'BarackObama': 4,\n",
    "    'NarendraModi': 5,\n",
    "    'ManmohanSingh': 6,\n",
    "    'VladimirPutin': 7}\n",
    "\n",
    "imfdb_dict = {'MadhuriDixit': 0,\n",
    "     'Kajol': 1,\n",
    "     'SharukhKhan': 2,\n",
    "     'ShilpaShetty': 3,\n",
    "     'AmitabhBachan': 4,\n",
    "     'KatrinaKaif': 5,\n",
    "     'AkshayKumar': 6,\n",
    "     'Amir': 7}\n",
    "\n",
    "# Load Image using PIL for dataset\n",
    "def load_image(path):\n",
    "    im = Image.open(path).convert('L' if opt['is_grayscale'] else 'RGB')\n",
    "    im = im.resize((opt['image_size'],opt['image_size']))\n",
    "    im = np.array(im)\n",
    "    im = im/256\n",
    "    return im\n",
    "\n",
    "# Load the full data from directory\n",
    "def load_data(dir_path):\n",
    "    image_list = []\n",
    "    y_list = []\n",
    "    \n",
    "    if \"CFW\" in dir_path:\n",
    "        label_dict = cfw_dict\n",
    "\n",
    "    elif \"yale\" in dir_path.lower():\n",
    "        label_dict = {}\n",
    "        for i in range(15):\n",
    "            label_dict[str(i+1)] = i\n",
    "    elif \"IMFDB\" in dir_path:\n",
    "        label_dict = imfdb_dict\n",
    "    else:\n",
    "        raise KeyError(\"Dataset not found.\")\n",
    "    \n",
    "    \n",
    "    for filename in sorted(os.listdir(dir_path)):\n",
    "        if filename.endswith(\".png\"):\n",
    "            im = load_image(os.path.join(dir_path,filename))\n",
    "            y = filename.split('_')[0]\n",
    "            y = label_dict[y] \n",
    "            image_list.append(im)\n",
    "            y_list.append(y)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    image_list = np.array(image_list)\n",
    "    y_list = np.array(y_list)\n",
    "\n",
    "    print(\"Dataset shape:\",image_list.shape)\n",
    "\n",
    "    return image_list,y_list\n",
    "\n",
    "# Display N Images in a nice format\n",
    "def disply_images(imgs,classes,row=1,col=2,w=64,h=64):\n",
    "    fig=plt.figure(figsize=(8, 8))\n",
    "    for i in range(1, col*row +1):\n",
    "        img = imgs[i-1]\n",
    "        fig.add_subplot(row, col, i)\n",
    "        \n",
    "        if opt['is_grayscale']:\n",
    "            plt.imshow(img , cmap='gray') \n",
    "        else:\n",
    "            plt.imshow(img)\n",
    "        \n",
    "        plt.title(\"Class:{}\".format(classes[i-1]))\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './datasets/Yale_face_database/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a1b262f8de58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Loading the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdirpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./datasets/Yale_face_database/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_grayscale'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b49ac13643fc>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(dir_path)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './datasets/Yale_face_database/'"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "dirpath = './datasets/Yale_face_database/'\n",
    "X,y = load_data(dirpath)\n",
    "N,H,W = X.shape[0:3]\n",
    "C = 1 if opt['is_grayscale'] else X.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample images\n",
    "ind = np.random.randint(0,y.shape[0],6)\n",
    "disply_images(X[ind,...],y[ind], row=2,col=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "dirpath1 = './datasets/IMFDB/'\n",
    "X1,y1 = load_data(dirpath1)\n",
    "N1,H1,W1 = X1.shape[0:3]\n",
    "C1 = 1 if opt['is_grayscale'] else X1.shape[3]\n",
    "# Show sample images\n",
    "ind = np.random.randint(0,y1.shape[0],6)\n",
    "disply_images(X1[ind,...],y1[ind], row=2,col=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "dirpath2 = './datasets/IIIT-CFW/'\n",
    "X2,y2 = load_data(dirpath2)\n",
    "N2,H2,W2 = X2.shape[0:3]\n",
    "C2 = 1 if opt['is_grayscale'] else X2.shape[3]\n",
    "# Show sample images\n",
    "ind = np.random.randint(0,y2.shape[0],6)\n",
    "disply_images(X2[ind,...],y2[ind], row=2,col=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features\n",
    "    You are provided 6 Features. These features are:\n",
    "   - Eigen Faces / PCA \n",
    "   - Kernel PCA\n",
    "   - Fisher Face / LDA\n",
    "   - Kernel Fisher Face\n",
    "   - VGG Features \n",
    "   - Resnet Features\n",
    "\n",
    "**VGG and Resnet features are last layer features learned by training a model for image classification**\n",
    "    \n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten to apply PCA/LDA\n",
    "X = X.reshape((N,H*W*C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = X1.reshape((N1,H1*W1*C1))\n",
    "X2 = X2.reshape((N2,H2*W2*C2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Eigen Face:\n",
    "Use principal component analysis to get the eigen faces. \n",
    "Go through the [documentation](!http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) on how to use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pca(X,k):\n",
    "    \"\"\"\n",
    "        Get PCA of K dimension using the top eigen vectors \n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=k)\n",
    "    # Projected data into k dimensions\n",
    "    X_k = pca.fit_transform(X)\n",
    "    # Reconstructed data from k dimensions\n",
    "    rec_X = pca.inverse_transform(X_k)\n",
    "    return X_k,rec_X,pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2. Kernel Face:\n",
    "Use Kernel principal component analysis to get the eigen faces. \n",
    "\n",
    "There are different kernels that can be used. Eg. Poly, rbf, sigmoid. Choose the whichever gives the best result or representation. See [link](!https://data-flair.training/blogs/svm-kernel-functions/) for better understanding of these kernels  \n",
    "\n",
    "Go through the [documentation](!https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA) on how to use it different kernels in Sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kernel_pca(X, k,kernel='rbf', degree=3):\n",
    "    \"\"\"\n",
    "        Get PCA of K dimension using the top eigen vectors \n",
    "        @param: X => Your data flattened to D dimension\n",
    "        @param: k => Number of components\n",
    "        @param: kernel => which kernel to use (“linear” | “poly” | “rbf” | “sigmoid” | “cosine” )\n",
    "        @param: d => Degree for poly kernels. Ignored by other kernels\n",
    "    \"\"\"\n",
    "    kpca = KernelPCA(n_components=k,kernel=kernel,degree=degree,remove_zero_eig=True)\n",
    "    X_k = kpca.fit_transform(X)\n",
    "    return X_k,kpca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fisher Face\n",
    "Another method similar to the eigenface technique is `fisherfaces` which uses linear discriminant analysis.\n",
    "This method for facial recognition is less sensitive to variation in lighting and pose of the face than using eigenfaces. Fisherface uses labelled data to retain more of the class-specific information during the dimension reduction stage.\n",
    "\n",
    "Go through the [documentation](!https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html) on how to use it different kernels in Sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lda(X,y, k):\n",
    "    \"\"\"\n",
    "        Get LDA of K dimension \n",
    "        @param: X => Your data flattened to D dimension\n",
    "        @param: k => Number of components\n",
    "    \"\"\"\n",
    "    lda = LDA(n_components=k)\n",
    "    X_k = lda.fit_transform(X,y)\n",
    "    return X_k,lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Kernel Fisher Face\n",
    "Use LDA using different kernels similiar to KernelPCA. Here the input is directly transformed instead of using the kernel trick.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kernel_lda(X,y,k,kernel='rbf',degree=3):\n",
    "    \"\"\"\n",
    "        Get LDA of K dimension \n",
    "        @param: X => Your data flattened to D dimension\n",
    "        @param: k => Number of components\n",
    "        @param: kernel => which kernel to use ( “poly” | “rbf” | “sigmoid”)\n",
    "    \"\"\"\n",
    "    # Transform  input\n",
    "    if kernel == \"poly\":\n",
    "        X_transformed = X**degree\n",
    "    elif kernel == \"rbf\":\n",
    "        var = np.var(X)\n",
    "        X_transformed= np.exp(-X/(2*var))\n",
    "    elif kernel == \"sigmoid\":\n",
    "        X_transformed = np.tanh(X)\n",
    "    else: \n",
    "        raise NotImplementedError(\"Kernel {} Not defined\".format(kernel))\n",
    "        \n",
    "    klda = LDA(n_components=k)\n",
    "    X_k = klda.fit_transform(X,y)\n",
    "    return X_k,klda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. VGG Features\n",
    "VGG Neural Networks a 19 layer CNN architecture introduced by Andrew Zisserman([Link](!https://arxiv.org/pdf/1409.1556.pdf) to paper). We are providing you with the last fully connected layer of this model.\n",
    "\n",
    "The model was trained for face classification on each dataset and each feature the dimension of 4096."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vgg_features(dirpath):\n",
    "    features = np.load(os.path.join(dirpath,\"VGG19_features.npy\"))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Resnet Features\n",
    "\n",
    "[Residual neural networks](!https://arxiv.org/pdf/1512.03385.pdf) are CNN with large depth, to effectively train these netwrorks they utilize skip connections, or short-cuts to jump over some layers. This helps solving [vanishing gradient problem](!https://en.wikipedia.org/wiki/Vanishing_gradient_problem) \n",
    "\n",
    "A 50 layer resnet model was trained for face classification on each dataset. Each feature the dimension of 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_features(dirpath):\n",
    "    features = np.load(os.path.join(dirpath,\"resnet50_features.npy\"))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1(a). What are eigen faces? \n",
    "\n",
    "___________________________\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1(b).  How many eigen vec-tors/faces are required to “satisfactorily” reconstruct a  person  in  these  three  datasets? (Don’t  forget  to make your argument based on eigen value spectrum) Show appropriate graphs, qualitative examples andmake a convincing argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute your features \n",
    "# eg.\n",
    "X_pca,_,_ = get_pca(X,165)\n",
    "X1_pca,_,_ = get_pca(X1,400)\n",
    "X2_pca,_,_ = get_pca(X2,672)\n",
    "print(X_pca.shape)\n",
    "print(X1_pca.shape)\n",
    "print(X2_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot  \n",
    "# eg.\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X_pca[:,0],X_pca[:,1],X_pca[:,2],c=y)\n",
    "plt.title(\"Yale\")\n",
    "plt.show()\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X1_pca[:,0],X1_pca[:,1],X1_pca[:,2],c=y1)\n",
    "plt.title(\"IMFDB\")\n",
    "plt.show()\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X2_pca[:,0],X2_pca[:,1],X2_pca[:,2],c=y2)\n",
    "plt.title(\"Yale\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the eigen value spectrum \n",
    "# CANCER\n",
    "def plot_eigen_spectrum(X,s):\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    cov = np.cov(X,rowvar=False)\n",
    "    print(cov.shape)\n",
    "    n=0\n",
    "    eigvals,_ = np.linalg.eigh(cov)\n",
    "    desc_eigvals = eigvals[::-1]\n",
    "    plt.plot(desc_eigvals,'bo',markersize=2)\n",
    "    plt.xlabel(\"Eigenvector\")\n",
    "    plt.ylabel(\"Eigenvalue\")\n",
    "    plt.title(s+' eigenspectrum')\n",
    "    plt.show()\n",
    "    significance = [np.abs(i)/np.sum(desc_eigvals) for i in desc_eigvals]\n",
    "    for i,val in enumerate(np.cumsum(significance)):\n",
    "        if val>=0.95:\n",
    "            n = i\n",
    "            print(val)\n",
    "            break\n",
    "        \n",
    "    #Plotting the Cumulative Summation of the Explained Variance\n",
    "    plt.figure()\n",
    "    plt.plot(np.cumsum(significance))\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Variance (%)') #for each component\n",
    "    plt.title(s+' Variance plot')\n",
    "    plt.show()\n",
    "    return n\n",
    "\n",
    "n = plot_eigen_spectrum(X_pca,\"Yale\")    \n",
    "n1 = plot_eigen_spectrum(X1_pca,\"IMFDB\")    \n",
    "n2 = plot_eigen_spectrum(X2_pca,\"IIIT \")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For 95% reconstruction, number of components :\n",
    "# print(\"Yale:\",n)\n",
    "# print(\"IMFDB\",n1)\n",
    "# print(\"IIIT-CNM\",n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if not running above two cells\n",
    "n = 65\n",
    "n1 = 128\n",
    "n2 = 360"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1(c). Reconstruct  the  image  back for each case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_images(X,k):\n",
    "    \"\"\"\n",
    "        Reconstruct the images back by just using the selected principal components. \n",
    "\n",
    "\n",
    "        You have to write the code in this code block.\n",
    "        You can change the functions provided above (eg, get_pca, get_lda) for your use case. \n",
    "            \n",
    "        @params: \n",
    "                Input parameters\n",
    "\n",
    "        @return reconstructed_X => reconstructed image\n",
    "        \n",
    "    \"\"\"\n",
    "    X_k, reconstruct_X,_ = get_pca(X,k)\n",
    "    print(reconstruct_X.shape)\n",
    "    \n",
    "    return X_k,reconstruct_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(n)\n",
    "X_pcaz,X_reconstructed = reconstruct_images(X,n)\n",
    "\n",
    "X_reconstruced_3d = X_reconstructed.reshape((N,H,W,C))\n",
    "# Display random images\n",
    "ind = np.random.randint(0,y.shape[0],6)\n",
    "disply_images(X_reconstruced_3d[ind,...],y[ind],row=2,col=3)\n",
    "\n",
    "# Show the reconstruction error\n",
    "print(np.sqrt(np.mean((X - X_reconstructed)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pcaz1,X_reconstructed1 = reconstruct_images(X1,n1)\n",
    "\n",
    "X_reconstruced_3d1 = X_reconstructed1.reshape((N1,H1,W1,C1))\n",
    "# Display random images\n",
    "ind = np.random.randint(0,y1.shape[0],6)\n",
    "disply_images(X_reconstruced_3d1[ind,...],y1[ind],row=2,col=3)\n",
    "\n",
    "# Show the reconstruction error\n",
    "print(np.sqrt(np.mean((X1 - X_reconstructed1)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pcaz2,X_reconstructed2 = reconstruct_images(X2,n2)\n",
    "\n",
    "X_reconstruced_3d2 = X_reconstructed2.reshape((N2,H2,W2,C2))\n",
    "# Display random images\n",
    "ind = np.random.randint(0,y2.shape[0],6)\n",
    "disply_images(X_reconstruced_3d2[ind,...],y2[ind],row=2,col=3)\n",
    "\n",
    "# Show the reconstruction error\n",
    "print(np.sqrt(np.mean((X2 - X_reconstructed2)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1(d). Which person/identity is difficult to represent com-pactly with fewer eigen vectors?  Why is that?  Explain with your empirical observations and intuitive answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here\n",
    "# Doing for Yale : \n",
    "print('Yale')\n",
    "pca = PCA(n_components=n)\n",
    "# Projected data into k dimensions\n",
    "X_k = pca.fit_transform(X)\n",
    "errs = []\n",
    "for i in range(8):\n",
    "    # get the projected class data\n",
    "    class_data = X_k[y==i]\n",
    "    # Reconstructed data from k dimensions\n",
    "    rec_class = pca.inverse_transform(class_data)\n",
    "    # Show the reconstruction error\n",
    "    er = np.sqrt(np.mean((X[y==i] - rec_class)**2))\n",
    "    print(\"Class reconstruction error\",i,\":\",er)\n",
    "    errs.append(er)\n",
    "errs = np.array(errs)\n",
    "max_err_class = np.argmax(errs)\n",
    "print(\"Max reconstruction error for class:\",max_err_class)\n",
    "\n",
    "# Doing for Imfdb : \n",
    "print('IMFDB')\n",
    "pca = PCA(n_components=n1)\n",
    "# Projected data into k dimensions\n",
    "X_k = pca.fit_transform(X1)\n",
    "errs1 = []\n",
    "for i in range(8):\n",
    "    # get the projected class data\n",
    "    class_data = X_k[y1==i]\n",
    "    # Reconstructed data from k dimensions\n",
    "    rec_class = pca.inverse_transform(class_data)\n",
    "    # Show the reconstruction error\n",
    "    er = np.sqrt(np.mean((X1[y1==i] - rec_class)**2))\n",
    "    print(\"Class reconstruction error\",i,\":\",er)\n",
    "    errs1.append(er)\n",
    "errs1 = np.array(errs1)\n",
    "max_err_class1 = np.argmax(errs1)\n",
    "print(\"Max reconstruction error for class:\",max_err_class1)   \n",
    "\n",
    "# Doing for IIIT-CFM : \n",
    "print('IIIT-CFM')\n",
    "pca = PCA(n_components=n2)\n",
    "# Projected data into k dimensions\n",
    "X_k = pca.fit_transform(X2)\n",
    "errs2 = []\n",
    "for i in range(8):\n",
    "    # get the projected class data\n",
    "    class_data = X_k[y2==i]\n",
    "    # Reconstructed data from k dimensions\n",
    "    rec_class = pca.inverse_transform(class_data)\n",
    "    # Show the reconstruction error\n",
    "    er = np.sqrt(np.mean((X2[y2==i] - rec_class)**2))\n",
    "    print(\"Class reconstruction error\",i,\":\",er)\n",
    "    errs2.append(er)\n",
    "errs2 = np.array(errs2)\n",
    "max_err_class2 = np.argmax(errs2)\n",
    "print(\"Max reconstruction error for class:\",max_err_class2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2(a). Use any classifier(MLP, Logistic regression, SVM, Decision Trees) and find the classification accuracy. \n",
    "\n",
    "2(b)Which method works well? Do a comparitivestudy. \n",
    "\n",
    "\n",
    "You already know the paper [Face Recognition Us-ing  Kernel  Methods](!http://face-rec.org/algorithms/Kernel/nips01.pdf) .See  this  as  an  example for empirical analysis of different features/classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier():\n",
    "    def __init__(self,hls,mi):\n",
    "        self.classifier = MLPClassifier(hidden_layer_sizes=hls, max_iter=mi,activation = 'relu',solver='adam',random_state=1)\n",
    "    \n",
    "    # Define your parameters eg, W,b, max_iterations etc. \n",
    "    \n",
    "    def classify(self,X):\n",
    "        \"\"\"\n",
    "            Given an input X classify it into appropriate class. \n",
    "        \"\"\"\n",
    "        y_pred = self.classifier.predict(X)\n",
    "        return y_pred\n",
    "        \n",
    "    def confusion_matrix(self,pred,y):\n",
    "        \"\"\"\n",
    "            A confusion matrix is a table that is often used to describe the performance of a classification\n",
    "            model (or “classifier”) on a set of test data for which the true values are known.\n",
    "            \n",
    "            \n",
    "            @return confusion_matrix => num_classesxnum_classes martix \n",
    "                where confusion_matrix[i,j] = number of prediction which are i and number of ground truth value equal j \n",
    "        \n",
    "        \"\"\"\n",
    "        cm = confusion_matrix(y,pred)\n",
    "        return cm\n",
    "        \n",
    "    def train(self,X_train,y_train):\n",
    "        \"\"\"\n",
    "            Given your training data, learn the parameters of your classifier\n",
    "            \n",
    "            @param X_train => NxD tensor. Where N is the number of samples and D is the dimension. \n",
    "                                it is the data on which your classifier will be trained. \n",
    "                                It can be any combination of features provided above.\n",
    "\n",
    "            @param y_train => N vector. Ground truth label \n",
    "    \n",
    "            @return Nothing\n",
    "        \"\"\"\n",
    "        self.classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "        \n",
    "    def validate(self,X_validate,y_validate):\n",
    "        \"\"\"\n",
    "            How good is the classifier on unseen data? Use the function below to calculate different metrics. \n",
    "            Based on these matrix change the hyperparmeters and judge the classification\n",
    "            \n",
    "            @param X_validate => NxD tensor. Where N is the number of samples and D is the dimension. \n",
    "                                it is the data on which your classifier validated. \n",
    "                                It can be any combination of features provided above.\n",
    "\n",
    "            @param y_validate => N vector. Ground truth label \n",
    "            \n",
    "        \"\"\"\n",
    "        predicted = self.classify(X_validate)\n",
    "\n",
    "        # Create a confusion matrix\n",
    "        confusion = self.confusion_matrix(predicted,y_validate)\n",
    "        \n",
    "        # Calculate Validation accuracy \n",
    "        val_accuracy = self.classifier.score(X_validate,y_validate)\n",
    "    \n",
    "        # Calculate precision and recall \n",
    "        precision = precision_score(y_validate,predicted,average='macro')\n",
    "        recall = recall_score(y_validate,predicted,average='macro')\n",
    "        # Calculate F1-score\n",
    "        f1 = f1_score(y_validate,predicted,average='macro')\n",
    "        \n",
    "        return predicted,confusion,val_accuracy,precision,recall,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train and validation split to train your classifier \n",
    "#Splitting the dataset into  training and validation sets\n",
    "X_train, X_test,y_train,y_test = train_test_split(X,y, test_size = 0.25, random_state = 21)\n",
    "X1_train, X1_test,y1_train,y1_test = train_test_split(X1,y1, test_size = 0.25, random_state = 21)\n",
    "X2_train, X2_test,y2_train,y2_test = train_test_split(X2,y2, test_size = 0.25, random_state = 21)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction for VGG and resnet\n",
    "X_vgg = get_vgg_features('./datasets/Yale_face_database/')\n",
    "X_res = get_resnet_features('./datasets/Yale_face_database/')\n",
    "X1_vgg = get_vgg_features('./datasets/IMFDB/')\n",
    "X1_res = get_resnet_features('./datasets/IMFDB/')\n",
    "X2_vgg = get_vgg_features('./datasets/IIIT-CFW/')\n",
    "X2_res = get_resnet_features('./datasets/IIIT-CFW/')\n",
    "\n",
    "X_train_vgg, X_test_vgg,y_train_vgg,y_test_vgg = train_test_split(X_vgg,y, test_size = 0.25, random_state = 21)\n",
    "X1_train_vgg, X1_test_vgg,y1_train_vgg,y1_test_vgg = train_test_split(X1_vgg,y1, test_size = 0.25, random_state = 21)\n",
    "X2_train_vgg, X2_test_vgg,y2_train_vgg,y2_test_vgg = train_test_split(X2_vgg,y2, test_size = 0.25, random_state = 21)\n",
    "\n",
    "X_train_res, X_test_res,y_train_res,y_test_res = train_test_split(X_res,y, test_size = 0.25, random_state = 21)\n",
    "X1_train_res, X1_test_res,y1_train_res,y1_test_res = train_test_split(X1_res,y1, test_size = 0.25, random_state = 21)\n",
    "X2_train_res, X2_test_res,y2_train_res,y2_test_res = train_test_split(X2_res,y2, test_size = 0.25, random_state = 21)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test sample on yale for pca using mlp\n",
    "X_train_pca,_,X_model_pca = get_pca(X_train,n)\n",
    "c = Classifier((1000,1000),350)\n",
    "c.train(X_train_pca,y_train)\n",
    "X_test_pca = X_model_pca.transform(X_test)\n",
    "predicted,confusion,val_accuracy,precision,recall,f1 = c.validate(X_test_pca,y_test)\n",
    "print(val_accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3 tables simiar to page-6 of the paper. One table per dataset \n",
    "# Each table will have 5 columns. \n",
    "# Feature/combination of feature used, reduced dimension space, classification error, accuracy, f1-score\n",
    "# Print the table. (You can use Pandas)\n",
    "\n",
    "feature_transforms = [get_pca,\\\n",
    "    get_kernel_pca,\\\n",
    "    get_lda,\\\n",
    "    get_kernel_lda]  \n",
    "components = [n,n,14,14]\n",
    "titles = [\"PCA\",'Kernel PCA(poly=3)','LDA','Kernel LDA(poly=3)']\n",
    "\n",
    "mat = []\n",
    "cfs = []\n",
    "for i,ft in enumerate(feature_transforms):\n",
    "    print(i,ft)\n",
    "    lst = []\n",
    "    if i==0:#pca\n",
    "        X_train_projected,_,X_model = ft(X_train,components[i])\n",
    "    elif i==1:#kpca\n",
    "        X_train_projected,X_model = ft(X_train,components[i])\n",
    "    else:#lda and klda\n",
    "        X_train_projected,X_model = ft(X_train,y_train,components[i])\n",
    "        \n",
    "    c = Classifier((1000,1000),350)\n",
    "    c.train(X_train_projected,y_train)\n",
    "    X_test_projected = X_model.transform(X_test)\n",
    "    predicted,confusion,val_accuracy,precision,recall,f1 = c.validate(X_test_projected,y_test)\n",
    "\n",
    "    lst.extend([titles[i],X_train_projected.shape[1],(1-val_accuracy)*100,val_accuracy*100,f1]) \n",
    "    mat.append(lst)\n",
    "    cfs.append(confusion)\n",
    "\n",
    "features =  [X_vgg,X_res]\n",
    "titles = ['VGG','Resnet']\n",
    "for i,ft in enumerate(features):\n",
    "    lst = []\n",
    "    training_data, validation_data, training_labels, validation_labels = train_test_split(ft,y, test_size = 0.2, random_state = 21)\n",
    "    c = Classifier((1000,1000),350)\n",
    "    c.train(training_data,training_labels)\n",
    "    predicted,confusion,val_accuracy,precision,recall,f1 = c.validate(validation_data,validation_labels)\n",
    "    lst.extend([titles[i],X_train_projected.shape[1],(1-val_accuracy)*100,val_accuracy*100,f1])   \n",
    "    mat.append(lst)\n",
    "    cfs.append(confusion)\n",
    "\n",
    "# LDA of VGG\n",
    "lst = []\n",
    "X_train_projected_vgg,X_model = get_lda(X_train_vgg,y_train_vgg,14)\n",
    "c = Classifier((1000,1000),350)\n",
    "c.train(X_train_projected_vgg,y_train_vgg)\n",
    "X_test_projected_vgg = X_model.transform(X_test_vgg)\n",
    "predicted,confusion,val_accuracy,precision,recall,f1 = c.validate(X_test_projected_vgg,y_test_vgg)\n",
    "\n",
    "lst.extend(['LDA of VGG',X_train_projected_vgg.shape[1],(1-val_accuracy)*100,val_accuracy*100,f1]) \n",
    "mat.append(lst)\n",
    "cfs.append(confusion)\n",
    "    \n",
    "# PCA of resnet\n",
    "lst = []\n",
    "X_train_projected_res,_,X_model = get_pca(X_train_res,n)\n",
    "c = Classifier((1000,1000),350)\n",
    "c.train(X_train_projected_res,y_train_res)\n",
    "X_test_projected_res = X_model.transform(X_test_res)\n",
    "predicted,confusion,val_accuracy,precision,recall,f1 = c.validate(X_test_projected_res,y_test_res)\n",
    "\n",
    "lst.extend(['PCA of resnet',X_train_projected_res.shape[1],(1-val_accuracy)*100,val_accuracy*100,f1]) \n",
    "mat.append(lst)\n",
    "cfs.append(confusion)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(mat)\n",
    "df.columns = ['Features/Combination of features','Reduced Dimensional Space','Classification error','Accuracy','F1 score']\n",
    "# Print the output.\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For imfdb\n",
    "feature_transforms = [get_pca,\\\n",
    "    get_kernel_pca,\\\n",
    "    get_lda,\\\n",
    "    get_kernel_lda]  \n",
    "components = [n1,n1,7,7]\n",
    "titles = [\"PCA\",'Kernel PCA(poly=3)','LDA','Kernel LDA(poly=3)']\n",
    "\n",
    "mat1 = []\n",
    "cfs1 = []\n",
    "for i,ft in enumerate(feature_transforms):\n",
    "    print(i,ft)\n",
    "    lst = []\n",
    "    if i==0:#pca\n",
    "        X1_train_projected,_,X1_model = ft(X1_train,components[i])\n",
    "    elif i==1:#kpca\n",
    "        X1_train_projected,X1_model = ft(X1_train,components[i])\n",
    "    else:#lda and klda\n",
    "        X1_train_projected,X1_model = ft(X1_train,y1_train,components[i])\n",
    "        \n",
    "    c = Classifier((1000,1000),350)\n",
    "    c.train(X1_train_projected,y1_train)\n",
    "    X1_test_projected = X1_model.transform(X1_test)\n",
    "    predicted,confusion,val_accuracy,precision,recall,f1 = c.validate(X1_test_projected,y1_test)\n",
    "\n",
    "    lst.extend([titles[i],X1_train_projected.shape[1],(1-val_accuracy)*100,val_accuracy*100,f1]) \n",
    "    mat1.append(lst)\n",
    "    cfs1.append(confusion)\n",
    "\n",
    "features =  [X1_vgg,X1_res]\n",
    "titles = ['VGG','Resnet']\n",
    "for i,ft in enumerate(features):\n",
    "    lst = []\n",
    "    training_data, validation_data, training_labels, validation_labels = train_test_split(ft,y1, test_size = 0.2, random_state = 21)\n",
    "    c = Classifier((1000,1000),350)\n",
    "    c.train(training_data,training_labels)\n",
    "    predicted,confusion,val_accuracy,precision,recall,f1 = c.validate(validation_data,validation_labels)\n",
    "    lst.extend([titles[i],X1_train_projected.shape[1],(1-val_accuracy)*100,val_accuracy*100,f1])   \n",
    "    mat1.append(lst)\n",
    "    cfs1.append(confusion)\n",
    "\n",
    "# LDA of VGG\n",
    "lst = []\n",
    "X1_train_projected_vgg,X1_model = get_lda(X1_train_vgg,y1_train_vgg,7)\n",
    "c = Classifier((1000,1000),350)\n",
    "c.train(X1_train_projected_vgg,y1_train_vgg)\n",
    "X1_test_projected_vgg = X1_model.transform(X1_test_vgg)\n",
    "predicted,confusion,val_accuracy,precision,recall,f1 = c.validate(X1_test_projected_vgg,y1_test_vgg)\n",
    "\n",
    "lst.extend(['LDA of VGG',X1_train_projected_vgg.shape[1],(1-val_accuracy)*100,val_accuracy*100,f1]) \n",
    "mat1.append(lst)\n",
    "cfs1.append(confusion)\n",
    "    \n",
    "# PCA of resnet\n",
    "lst = []\n",
    "X1_train_projected_res,_,X1_model = get_pca(X1_train_res,n)\n",
    "c = Classifier((1000,1000),350)\n",
    "c.train(X1_train_projected_res,y1_train_res)\n",
    "X1_test_projected_res = X1_model.transform(X1_test_res)\n",
    "predicted,confusion,val_accuracy,precision,recall,f1 = c.validate(X1_test_projected_res,y1_test_res)\n",
    "\n",
    "lst.extend(['PCA of resnet',X1_train_projected_res.shape[1],(1-val_accuracy)*100,val_accuracy*100,f1]) \n",
    "mat1.append(lst)\n",
    "cfs1.append(confusion)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(mat1)\n",
    "df.columns = ['Features/Combination of features','Reduced Dimensional Space','Classification error','Accuracy','F1 score']\n",
    "# Print the output.\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For iiit-cfm\n",
    "feature_transforms = [get_pca,\\\n",
    "    get_kernel_pca,\\\n",
    "    get_lda,\\\n",
    "    get_kernel_lda]  \n",
    "components = [n2,n2,7,7]\n",
    "titles = [\"PCA\",'Kernel PCA(poly=3)','LDA','Kernel LDA(poly=3)']\n",
    "\n",
    "mat2 = []\n",
    "cfs2 = []\n",
    "for i,ft in enumerate(feature_transforms):\n",
    "    print(i,ft)\n",
    "    lst = []\n",
    "    if i==0:#pca\n",
    "        X2_train_projected,_,X2_model = ft(X2_train,components[i])\n",
    "    elif i==1:#kpca\n",
    "        X2_train_projected,X2_model = ft(X2_train,components[i])\n",
    "    else:#lda and klda\n",
    "        X2_train_projected,X2_model = ft(X2_train,y2_train,components[i])\n",
    "        \n",
    "    c = Classifier((1000,1000),350)\n",
    "    c.train(X2_train_projected,y2_train)\n",
    "    X2_test_projected = X2_model.transform(X2_test)\n",
    "    predicted,confusion,val_accuracy,precision,recall,f1 = c.validate(X2_test_projected,y2_test)\n",
    "\n",
    "    lst.extend([titles[i],X2_train_projected.shape[1],(1-val_accuracy)*100,val_accuracy*100,f1]) \n",
    "    mat2.append(lst)\n",
    "    cfs2.append(confusion)\n",
    "\n",
    "features =  [X2_vgg,X2_res]\n",
    "titles = ['VGG','Resnet']\n",
    "for i,ft in enumerate(features):\n",
    "    lst = []\n",
    "    training_data, validation_data, training_labels, validation_labels = train_test_split(ft,y2, test_size = 0.2, random_state = 21)\n",
    "    c = Classifier((1000,1000),350)\n",
    "    c.train(training_data,training_labels)\n",
    "    predicted,confusion,val_accuracy,precision,recall,f1 = c.validate(validation_data,validation_labels)\n",
    "    lst.extend([titles[i],X2_train_projected.shape[1],(1-val_accuracy)*100,val_accuracy*100,f1])   \n",
    "    mat2.append(lst)\n",
    "    cfs2.append(confusion)\n",
    "\n",
    "# LDA of VGG\n",
    "lst = []\n",
    "X2_train_projected_vgg,X2_model = get_lda(X2_train_vgg,y2_train_vgg,7)\n",
    "c = Classifier((1000,1000),350)\n",
    "c.train(X2_train_projected_vgg,y2_train_vgg)\n",
    "X2_test_projected_vgg = X2_model.transform(X2_test_vgg)\n",
    "predicted,confusion,val_accuracy,precision,recall,f1 = c.validate(X2_test_projected_vgg,y2_test_vgg)\n",
    "\n",
    "lst.extend(['LDA of VGG',X2_train_projected_vgg.shape[1],(1-val_accuracy)*100,val_accuracy*100,f1]) \n",
    "mat2.append(lst)\n",
    "cfs2.append(confusion)\n",
    "    \n",
    "# PCA of resnet\n",
    "lst = []\n",
    "X2_train_projected_res,_,X2_model = get_pca(X2_train_res,n)\n",
    "c = Classifier((1000,1000),350)\n",
    "c.train(X2_train_projected_res,y2_train_res)\n",
    "X2_test_projected_res = X2_model.transform(X2_test_res)\n",
    "predicted,confusion,val_accuracy,precision,recall,f1 = c.validate(X2_test_projected_res,y2_test_res)\n",
    "\n",
    "lst.extend(['PCA of resnet',X2_train_projected_res.shape[1],(1-val_accuracy)*100,val_accuracy*100,f1]) \n",
    "mat2.append(lst)\n",
    "cfs2.append(confusion)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(mat2)\n",
    "df.columns = ['Features/Combination of features','Reduced Dimensional Space','Classification error','Accuracy','F1 score']\n",
    "# Print the output.\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each dataset print the confusion matrix for the best model \n",
    "def print_heatmap(cm,d,s):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.heatmap(cm,annot=True)\n",
    "    plt.ylabel('Actual label');\n",
    "    plt.xlabel('Predicted label');\n",
    "    plt.title(d+' Confusion matrix '+s, size = 10)\n",
    "    plt.show()\n",
    "    \n",
    "print(\"For Yale, Resnet gave best results\")\n",
    "print_heatmap(cfs[5],'Yale','from Resnet')\n",
    "print(\"For IMFDB,  PCA of Resnet gave best results\")\n",
    "print_heatmap(cfs1[7],'Imfdb','from LDA of VGG')\n",
    "print(\"For IIIT-CFM, PCA of Resnet gave best results\")\n",
    "print_heatmap(cfs2[7],'IIIT-CFM','from PCA of Resnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Similiar to 1(b) use t-SNE based visilization of faces?  Does it makesense?  Do you see similar people coming together?or something else?  Can you do visualization datasetwise and combined? Here you will use a popular implementation.(Worth  reading and understanding  t-SNE.  We  will not discuss it in the class and out of scope for thiscourse/exams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TSNE for different features and create a scatter plot\n",
    "def TSNE_features(data,y,k,s):\n",
    "    print(s)\n",
    "    # Compute\n",
    "    X_TSNE = TSNE(n_components=k).fit_transform(data)\n",
    "    print(\"Reduced dimensions:\",X_TSNE.shape)\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    # Plot the representation in 2d/3d\n",
    "    ax.scatter(X_TSNE[:,0],X_TSNE[:,1],X_TSNE[:,2],c=y)\n",
    "    plt.title(s)\n",
    "    plt.show()\n",
    "\n",
    "titles = [\"TSNE on Yale\",\"TSNE on IMFDB\",\"TSNE on IIIT-CFM\"]\n",
    "ys = [y,y1,y2]\n",
    "for i,p in enumerate([X_pcaz,X_pcaz1,X_pcaz2]):\n",
    "    TSNE_features(p,ys[i],3,titles[i]+\" PCA\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX = np.vstack((X,X1,X2))\n",
    "yy = np.vstack((y.reshape(165,1),(y1+15).reshape(400,1),(y2+15+8).reshape(672,1)))\n",
    "print(yy.shape)\n",
    "print(XX.shape)\n",
    "yy = yy.reshape(1237)\n",
    "lda_XX = get_lda(XX,yy,30)[0]\n",
    "TSNE_features(lda_XX,yy,3,\"TSNE on all data LDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldas = [get_lda(X,y,14)[0],get_lda(X1,y1,7)[0],get_lda(X2,y2,7)[0]]\n",
    "for i,p in enumerate(ldas):\n",
    "    TSNE_features(p,ys[i],3,\"TSNE on all data LDA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.`face`  is  used  for  verification.   \n",
    "\n",
    "4(a) How do we formulate the problem using KNN \n",
    "\n",
    "4(b) How do we analyze the performance ? suggest  the  metrics  (like  accuracy) that is appropriate for this task.\n",
    "\n",
    "_______________________________________________________________________\n",
    "\n",
    "4(c)Show empirical re-sults  with  all  the  representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceVerification():\n",
    "    def __init__(self,k):\n",
    "        self.knn = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "    # Define your parameters eg, W,b, max_iterations etc. \n",
    "    \n",
    "    def verify(self,X,y):\n",
    "        \"\"\"\n",
    "            Given an input X find if the class id is correct or not.\n",
    "            \n",
    "            @return verfication_results => N vector containing True or False. \n",
    "                    If the class-id matches with your prediction then true else false.   \n",
    "        \"\"\"\n",
    "        predictions = self.knn.predict(X)\n",
    "        results = (predictions==y)\n",
    "        return results,predictions\n",
    "\n",
    "    def train(self,X_train,y_train):\n",
    "        \"\"\"\n",
    "            Given your training data, learn the parameters of your classifier\n",
    "            \n",
    "            @param X_train => NxD tensor. Where N is the number of samples and D is the dimension. \n",
    "                                it is the data on which your verification system will be trained. \n",
    "                                It can be any combination of features provided above.\n",
    "\n",
    "            @param y_train => N vector. Ground truth label \n",
    "    \n",
    "            @return Nothing\n",
    "        \"\"\"\n",
    "        self.knn.fit(X_train, y_train)\n",
    "        \n",
    "    def validate(self,X_validate,y_validate):\n",
    "        \"\"\"\n",
    "            How good is your system on unseen data? Use the function below to calculate different metrics. \n",
    "            Based on these matrix change the hyperparmeters\n",
    "            \n",
    "            @param X_validate => NxD tensor. Where N is the number of samples and D is the dimension. \n",
    "                                It can be any combination of features provided above.\n",
    "\n",
    "            @param y_validate => N vector. Ground truth label \n",
    "            \n",
    "        \"\"\"\n",
    "        accuracy = self.knn.score(X_validate,y_validate)*100\n",
    "        error = 100 - accuracy\n",
    "        precision = precision_score(y_validate,self.verify(X_validate,y_validate)[1],average='macro')\n",
    "        return accuracy, error, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train and validation split and show your results\n",
    "X_train, X_test,y_train,y_test = train_test_split(X,y, test_size = 0.25, random_state = 21)\n",
    "X1_train, X1_test,y1_train,y1_test = train_test_split(X1,y1, test_size = 0.25, random_state = 21)\n",
    "X2_train, X2_test,y2_train,y2_test = train_test_split(X2,y2, test_size = 0.25, random_state = 21)\n",
    "# Feature extraction for VGG and resnet\n",
    "X_vgg = get_vgg_features('./datasets/Yale_face_database/')\n",
    "X_res = get_resnet_features('./datasets/Yale_face_database/')\n",
    "X1_vgg = get_vgg_features('./datasets/IMFDB/')\n",
    "X1_res = get_resnet_features('./datasets/IMFDB/')\n",
    "X2_vgg = get_vgg_features('./datasets/IIIT-CFW/')\n",
    "X2_res = get_resnet_features('./datasets/IIIT-CFW/')\n",
    "\n",
    "X_train_vgg, X_test_vgg,y_train_vgg,y_test_vgg = train_test_split(X_vgg,y, test_size = 0.25, random_state = 21)\n",
    "X1_train_vgg, X1_test_vgg,y1_train_vgg,y1_test_vgg = train_test_split(X1_vgg,y1, test_size = 0.25, random_state = 21)\n",
    "X2_train_vgg, X2_test_vgg,y2_train_vgg,y2_test_vgg = train_test_split(X2_vgg,y2, test_size = 0.25, random_state = 21)\n",
    "\n",
    "X_train_res, X_test_res,y_train_res,y_test_res = train_test_split(X_res,y, test_size = 0.25, random_state = 21)\n",
    "X1_train_res, X1_test_res,y1_train_res,y1_test_res = train_test_split(X1_res,y1, test_size = 0.25, random_state = 21)\n",
    "X2_train_res, X2_test_res,y2_train_res,y2_test_res = train_test_split(X2_res,y2, test_size = 0.25, random_state = 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3 tables simiar to page-6 of the paper. One table per dataset \n",
    "# Each table will have 5 columns. \n",
    "# Feature/combination of feature used, reduced dimension space, verification error, accuracy, precision\n",
    "feature_transforms = [get_pca,\\\n",
    "    get_kernel_pca,\\\n",
    "    get_lda,\\\n",
    "    get_kernel_lda ]  \n",
    "components = [n,n,14,14]\n",
    "titles = [\"PCA\",'Kernel PCA(poly=3)','LDA','Kernel LDA(poly=3)']\n",
    "\n",
    "mat = []\n",
    "cfs = []\n",
    "for i,ft in enumerate(feature_transforms):\n",
    "    print(i,ft)\n",
    "    lst = []\n",
    "    if i==0:#pca\n",
    "        X_train_projected,_,X_model = ft(X_train,components[i])\n",
    "    elif i==1:#kpca\n",
    "        X_train_projected,X_model = ft(X_train,components[i])\n",
    "    else:#lda and klda\n",
    "        X_train_projected,X_model = ft(X_train,y_train,components[i])\n",
    "        \n",
    "    c = FaceVerification(5)\n",
    "    c.train(X_train_projected,y_train)\n",
    "    X_test_projected = X_model.transform(X_test)\n",
    "    val_accuracy,error,precision = c.validate(X_test_projected,y_test)\n",
    "\n",
    "    lst.extend([titles[i],X_train_projected.shape[1],val_accuracy,error,precision]) \n",
    "    mat.append(lst)\n",
    "    cfs.append(confusion)\n",
    "\n",
    "features =  [X_vgg,X_res]\n",
    "titles = ['VGG','Resnet']\n",
    "for i,ft in enumerate(features):\n",
    "    lst = []\n",
    "    training_data, validation_data, training_labels, validation_labels = train_test_split(ft,y, test_size = 0.2, random_state = 21)\n",
    "    c = FaceVerification(5)\n",
    "    c.train(training_data,training_labels)\n",
    "    val_accuracy,error,precision = c.validate(validation_data,validation_labels)\n",
    "    lst.extend([titles[i],X_train_projected.shape[1],val_accuracy,error,precision])   \n",
    "    mat.append(lst)\n",
    "    cfs.append(confusion)\n",
    "\n",
    "# LDA of VGG\n",
    "lst = []\n",
    "X_train_projected_vgg,X_model = get_lda(X_train_vgg,y_train_vgg,14)\n",
    "c = FaceVerification(5)\n",
    "c.train(X_train_projected_vgg,y_train_vgg)\n",
    "X_test_projected_vgg = X_model.transform(X_test_vgg)\n",
    "val_accuracy,error,precision = c.validate(X_test_projected_vgg,y_test_vgg)\n",
    "\n",
    "lst.extend(['LDA of VGG',X_train_projected_vgg.shape[1],val_accuracy,error,precision]) \n",
    "mat.append(lst)\n",
    "cfs.append(confusion)\n",
    "    \n",
    "# PCA of resnet\n",
    "lst = []\n",
    "X_train_projected_res,_,X_model = get_pca(X_train_res,n)\n",
    "c = FaceVerification(5)\n",
    "c.train(X_train_projected_res,y_train_res)\n",
    "X_test_projected_res = X_model.transform(X_test_res)\n",
    "val_accuracy,error,precision = c.validate(X_test_projected_res,y_test_res)\n",
    "\n",
    "lst.extend(['PCA of resnet',X_train_projected_res.shape[1],val_accuracy,error,precision]) \n",
    "mat.append(lst)\n",
    "cfs.append(confusion)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(mat)\n",
    "df.columns = ['Features/Combination of features','Reduced Dimensional Space','Accuracy','Classification error','Precision']\n",
    "# Print the output.\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "for t in [3,5,7,9,12]:\n",
    "    X_train_projected,X_model = get_lda(X_train,y_train,14)\n",
    "    c = FaceVerification(t)\n",
    "    c.train(X_train_projected,y_train)\n",
    "    X_test_projected = X_model.transform(X_test)\n",
    "    val_accuracy,error,precision = c.validate(X_test_projected,y_test)\n",
    "    lst.append([t,val_accuracy,precision])    \n",
    "df = pd.DataFrame(lst)\n",
    "df.columns = ['K','Accuracy','Precision']\n",
    "# Print the output.\n",
    "print(df.to_string(index=False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3 tables simiar to page-6 of the paper. One table per dataset \n",
    "# Each table will have 5 columns. \n",
    "# Feature/combination of feature used, reduced dimension space, verification error, accuracy, precision\n",
    "feature_transforms = [get_pca,\\\n",
    "    get_kernel_pca,\\\n",
    "    get_lda,\\\n",
    "    get_kernel_lda]  \n",
    "components = [n1,n1,7,7]\n",
    "titles = [\"PCA\",'Kernel PCA(poly=3)','LDA','Kernel LDA(poly=3)']\n",
    "\n",
    "mat = []\n",
    "cfs = []\n",
    "for i,ft in enumerate(feature_transforms):\n",
    "    print(i,ft)\n",
    "    lst = []\n",
    "    if i==0:#pca\n",
    "        X1_train_projected,_,X1_model = ft(X1_train,components[i])\n",
    "    elif i==1:#kpca\n",
    "        X1_train_projected,X1_model = ft(X1_train,components[i])\n",
    "    else:#lda and klda\n",
    "        X1_train_projected,X1_model = ft(X1_train,y1_train,components[i])\n",
    "        \n",
    "    c = FaceVerification(5)\n",
    "    c.train(X1_train_projected,y1_train)\n",
    "    X1_test_projected = X1_model.transform(X1_test)\n",
    "    val_accuracy,error,precision = c.validate(X1_test_projected,y1_test)\n",
    "\n",
    "    lst.extend([titles[i],X1_train_projected.shape[1],val_accuracy,error,precision]) \n",
    "    mat.append(lst)\n",
    "    cfs.append(confusion)\n",
    "\n",
    "features =  [X1_vgg,X1_res]\n",
    "titles = ['VGG','Resnet']\n",
    "for i,ft in enumerate(features):\n",
    "    lst = []\n",
    "    training_data, validation_data, training_labels, validation_labels = train_test_split(ft,y1, test_size = 0.2, random_state = 21)\n",
    "    c = FaceVerification(5)\n",
    "    c.train(training_data,training_labels)\n",
    "    val_accuracy,error,precision = c.validate(validation_data,validation_labels)\n",
    "    lst.extend([titles[i],X1_train_projected.shape[1],val_accuracy,error,precision])   \n",
    "    mat.append(lst)\n",
    "    cfs.append(confusion)\n",
    "\n",
    "# LDA of VGG\n",
    "lst = []\n",
    "X1_train_projected_vgg,X1_model = get_lda(X1_train_vgg,y1_train_vgg,14)\n",
    "c = FaceVerification(5)\n",
    "c.train(X1_train_projected_vgg,y1_train_vgg)\n",
    "X1_test_projected_vgg = X1_model.transform(X1_test_vgg)\n",
    "val_accuracy,error,precision = c.validate(X1_test_projected_vgg,y1_test_vgg)\n",
    "\n",
    "lst.extend(['LDA of VGG',X1_train_projected_vgg.shape[1],val_accuracy,error,precision]) \n",
    "mat.append(lst)\n",
    "cfs.append(confusion)\n",
    "    \n",
    "# PCA of resnet\n",
    "lst = []\n",
    "X1_train_projected_res,_,X1_model = get_pca(X1_train_res,n)\n",
    "c = FaceVerification(5)\n",
    "c.train(X1_train_projected_res,y1_train_res)\n",
    "X1_test_projected_res = X1_model.transform(X1_test_res)\n",
    "val_accuracy,error,precision = c.validate(X1_test_projected_res,y1_test_res)\n",
    "\n",
    "lst.extend(['PCA of resnet',X1_train_projected_res.shape[1],val_accuracy,error,precision]) \n",
    "mat.append(lst)\n",
    "cfs.append(confusion)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(mat)\n",
    "df.columns = ['Features/Combination of features','Reduced Dimensional Space','Accuracy','Classification error','Precision']\n",
    "# Print the output.\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "for t in [3,5,7,9,12]:\n",
    "    X1_train_projected,X1_model = get_lda(X1_train,y1_train,14)\n",
    "    c = FaceVerification(t)\n",
    "    c.train(X1_train_projected,y1_train)\n",
    "    X1_test_projected = X1_model.transform(X1_test)\n",
    "    val_accuracy,error,precision = c.validate(X1_test_projected,y1_test)\n",
    "    lst.append([t,val_accuracy,precision])    \n",
    "df = pd.DataFrame(lst)\n",
    "df.columns = ['K','Accuracy','Precision']\n",
    "# Print the output.\n",
    "print(df.to_string(index=False))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3 tables simiar to page-6 of the paper. One table per dataset \n",
    "# Each table will have 5 columns. \n",
    "# Feature/combination of feature used, reduced dimension space, verification error, accuracy, precision\n",
    "feature_transforms = [get_pca,\\\n",
    "    get_kernel_pca,\\\n",
    "    get_lda,\\\n",
    "    get_kernel_lda]  \n",
    "components = [n2,n2,7,7]\n",
    "titles = [\"PCA\",'Kernel PCA(poly=3)','LDA','Kernel LDA(poly=3)']\n",
    "\n",
    "mat = []\n",
    "cfs = []\n",
    "for i,ft in enumerate(feature_transforms):\n",
    "    print(i,ft)\n",
    "    lst = []\n",
    "    if i==0:#pca\n",
    "        X2_train_projected,_,X2_model = ft(X2_train,components[i])\n",
    "    elif i==1:#kpca\n",
    "        X2_train_projected,X2_model = ft(X2_train,components[i])\n",
    "    else:#lda and klda\n",
    "        X2_train_projected,X2_model = ft(X2_train,y2_train,components[i])\n",
    "        \n",
    "    c = FaceVerification(5)\n",
    "    c.train(X2_train_projected,y2_train)\n",
    "    X2_test_projected = X2_model.transform(X2_test)\n",
    "    val_accuracy,error,precision = c.validate(X2_test_projected,y2_test)\n",
    "\n",
    "    lst.extend([titles[i],X2_train_projected.shape[1],val_accuracy,error,precision]) \n",
    "    mat.append(lst)\n",
    "    cfs.append(confusion)\n",
    "\n",
    "features =  [X2_vgg,X2_res]\n",
    "titles = ['VGG','Resnet']\n",
    "for i,ft in enumerate(features):\n",
    "    lst = []\n",
    "    training_data, validation_data, training_labels, validation_labels = train_test_split(ft,y2, test_size = 0.2, random_state = 21)\n",
    "    c = FaceVerification(5)\n",
    "    c.train(training_data,training_labels)\n",
    "    val_accuracy,error,precision = c.validate(validation_data,validation_labels)\n",
    "    lst.extend([titles[i],X2_train_projected.shape[1],val_accuracy,error,precision])   \n",
    "    mat.append(lst)\n",
    "    cfs.append(confusion)\n",
    "\n",
    "# LDA of VGG\n",
    "lst = []\n",
    "X2_train_projected_vgg,X2_model = get_lda(X2_train_vgg,y2_train_vgg,14)\n",
    "c = FaceVerification(5)\n",
    "c.train(X2_train_projected_vgg,y2_train_vgg)\n",
    "X2_test_projected_vgg = X2_model.transform(X2_test_vgg)\n",
    "val_accuracy,error,precision = c.validate(X2_test_projected_vgg,y2_test_vgg)\n",
    "\n",
    "lst.extend(['LDA of VGG',X2_train_projected_vgg.shape[1],val_accuracy,error,precision]) \n",
    "mat.append(lst)\n",
    "cfs.append(confusion)\n",
    "    \n",
    "# PCA of resnet\n",
    "lst = []\n",
    "X2_train_projected_res,_,X2_model = get_pca(X2_train_res,n)\n",
    "c = FaceVerification(5)\n",
    "c.train(X2_train_projected_res,y2_train_res)\n",
    "X2_test_projected_res = X2_model.transform(X2_test_res)\n",
    "val_accuracy,error,precision = c.validate(X2_test_projected_res,y2_test_res)\n",
    "\n",
    "lst.extend(['PCA of resnet',X2_train_projected_res.shape[1],val_accuracy,error,precision]) \n",
    "mat.append(lst)\n",
    "cfs.append(confusion)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(mat)\n",
    "df.columns = ['Features/Combination of features','Reduced Dimensional Space','Accuracy','Classification error','Precision']\n",
    "# Print the output.\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "for t in [3,5,7,9,12]:\n",
    "    X2_train_projected,X2_model = get_lda(X2_train,y2_train,14)\n",
    "    c = FaceVerification(t)\n",
    "    c.train(X2_train_projected,y2_train)\n",
    "    X2_test_projected = X2_model.transform(X2_test)\n",
    "    val_accuracy,error,precision = c.validate(X2_test_projected,y2_test)\n",
    "    lst.append([t,val_accuracy,precision])    \n",
    "df = pd.DataFrame(lst)\n",
    "df.columns = ['K','Accuracy','Precision']\n",
    "# Print the output.\n",
    "print(df.to_string(index=False))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extenstion / Application\n",
    "    Create a system for any one of the following problems:\n",
    "\n",
    "- Politicians  vs  Filmstars  in  a  public  data  set.   (eg.LFW)\n",
    "        You already have seen IIIT-CFW dataset. Use it for classification. \n",
    "- Age prediction\n",
    "        Given different actors/actress in IMFDB create new labels based on their age.  \n",
    "- Gender prediction\n",
    "        Given different actors/actress in IMFDB+IIIT-CFW create new labels based on their gender.\n",
    "- Emotion classification\n",
    "        Both the yale dataset and IMFDB contain an `emotion.txt` file. Using that you can create a emotion predicter \n",
    "- cartoon vs real images\n",
    "        Use a combination of IIIT-CFW and other dataset. \n",
    "        \n",
    "\n",
    "\n",
    "You are free to use a new dataset that is publicly avail-able or even create one by crawling from internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# Load the full data from directory\n",
    "\n",
    "def load_data(dirpath,fname):\n",
    "    emo_dict = dict()\n",
    "    with open(fname) as f:\n",
    "        content = f.readlines()\n",
    "        names = [x.strip().split('.')[0] for x in content]\n",
    "        emotions = [x.strip().split('.')[1].split(',')[1] for x in content]\n",
    "    for i,name in enumerate(names):    \n",
    "        emo_dict[name] = emotions[i]\n",
    "\n",
    "    y_list = []\n",
    "    image_list = []\n",
    "    i=0\n",
    "    for filename in sorted(os.listdir(dirpath)):\n",
    "        if filename.endswith(\".png\"):\n",
    "            im = load_image(os.path.join(dirpath,filename))\n",
    "            y = filename.split('.')[0]\n",
    "            # print(y,end=' ')\n",
    "            y = emo_dict[y] \n",
    "            image_list.append(im)\n",
    "            y_list.append(y)\n",
    "            # print(y)\n",
    "        else:\n",
    "            continue\n",
    "    image_list = np.array(image_list)\n",
    "    y_list = np.array(y_list)\n",
    "\n",
    "    print(\"Dataset shape:\",image_list.shape)\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(np.unique(y_list))\n",
    "    y_list_enc = le.transform(y_list)\n",
    "\n",
    "    return image_list,y_list_enc,le\n",
    "\n",
    "# X1,y1\n",
    "print('Imfdb')\n",
    "dirpath1 = './datasets/IMFDB/'   \n",
    "X1_data,y1,le1 = load_data(dirpath1,dirpath1+'emotion.txt')\n",
    "X1 = X1_data.reshape((N1,H1*W1*C1))\n",
    "print(\"Linearized shape:\",X1.shape)\n",
    "# X,y\n",
    "print('Yale')\n",
    "dirpath = './datasets/Yale_face_database/'   \n",
    "X_data,y,le = load_data(dirpath,dirpath+'emotion.txt')\n",
    "X = X_data.reshape((N,H*W*C))\n",
    "print(\"Linearized shape:\",X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your features\n",
    "# Do train test split\n",
    "X_train, X_test,y_train,y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)\n",
    "X1_train, X1_test,y1_train,y1_test = train_test_split(X1,y1, test_size = 0.2, random_state = 42)\n",
    "# Feature extraction for VGG and resnet\n",
    "X_vgg = get_vgg_features('./datasets/Yale_face_database/')\n",
    "X_res = get_resnet_features('./datasets/Yale_face_database/')\n",
    "X1_vgg = get_vgg_features('./datasets/IMFDB/')\n",
    "X1_res = get_resnet_features('./datasets/IMFDB/')\n",
    "\n",
    "X_train_vgg, X_test_vgg,y_train_vgg,y_test_vgg = train_test_split(X_vgg,y, test_size = 0.2, random_state = 42)\n",
    "X1_train_vgg, X1_test_vgg,y1_train_vgg,y1_test_vgg = train_test_split(X1_vgg,y1, test_size = 0.2, random_state = 42)\n",
    "\n",
    "X_train_res, X_test_res,y_train_res,y_test_res = train_test_split(X_res,y, test_size = 0.2, random_state = 42)\n",
    "X1_train_res, X1_test_res,y1_train_res,y1_test_res = train_test_split(X1_res,y1, test_size = 0.2, random_state = 42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogClassifier():\n",
    "    def __init__(self):\n",
    "        # self.classifier = LogisticRegression(C=10, solver='lbfgs',max_iter=1000,multi_class='multinomial',penalty='l2')\n",
    "        self.classifier = LogisticRegression(C=0.01)\n",
    "    def classify(self,X):\n",
    "        y_pred = self.classifier.predict(X)\n",
    "        return y_pred    \n",
    "    def confusion_matrix(self,pred,y):\n",
    "        cm = confusion_matrix(y,pred)\n",
    "        return cm        \n",
    "    def train(self,X_train,y_train):\n",
    "        self.classifier.fit(X_train, y_train)     \n",
    "    def validate(self,X_validate,y_validate):\n",
    "        predicted = self.classify(X_validate)\n",
    "        confusion = self.confusion_matrix(predicted,y_validate)\n",
    "        val_accuracy = self.classifier.score(X_validate,y_validate)\n",
    "        precision = precision_score(y_validate,predicted,average='macro')\n",
    "        recall = recall_score(y_validate,predicted,average='macro')\n",
    "        f1 = f1_score(y_validate,predicted,average='macro')\n",
    "        return predicted,confusion,val_accuracy,precision,recall,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_transforms = [get_pca,\\\n",
    "    get_kernel_pca,\\\n",
    "    get_lda,\\\n",
    "    get_kernel_lda]  \n",
    "components = [n,n,10,10]\n",
    "titles = [\"PCA\",'Kernel PCA(poly=3)','LDA','Kernel LDA(poly=3)']\n",
    "\n",
    "mat = []\n",
    "cfs = []\n",
    "for i,ft in enumerate(feature_transforms):\n",
    "    print(i,ft)\n",
    "    lst = []\n",
    "    if i==0:#pca\n",
    "        X_train_projected,_,X_model = ft(X_train,components[i])\n",
    "    elif i==1:#kpca\n",
    "        X_train_projected,X_model = ft(X_train,components[i])\n",
    "    else:#lda and klda\n",
    "        X_train_projected,X_model = ft(X_train,y_train,components[i])\n",
    "        \n",
    "    c = LogClassifier()\n",
    "    c.train(X_train_projected,y_train)\n",
    "    X_test_projected = X_model.transform(X_test)\n",
    "    predicted,confusion,val_accuracy,precision,recall,f1 = c.validate(X_test_projected,y_test)\n",
    "\n",
    "    lst.extend([titles[i],X_train_projected.shape[1],(1-val_accuracy)*100,val_accuracy*100,f1]) \n",
    "    mat.append(lst)\n",
    "    cfs.append(confusion)\n",
    "\n",
    "features =  [X_vgg,X_res]\n",
    "titles = ['VGG','Resnet']\n",
    "for i,ft in enumerate(features):\n",
    "    lst = []\n",
    "    training_data, validation_data, training_labels, validation_labels = train_test_split(ft,y, test_size = 0.2, random_state = 21)\n",
    "    c = LogClassifier()\n",
    "    c.train(training_data,training_labels)\n",
    "    predicted,confusion,val_accuracy,precision,recall,f1 = c.validate(validation_data,validation_labels)\n",
    "    lst.extend([titles[i],X_train_projected.shape[1],(1-val_accuracy)*100,val_accuracy*100,f1])   \n",
    "    mat.append(lst)\n",
    "    cfs.append(confusion)\n",
    "\n",
    "# LDA of VGG\n",
    "lst = []\n",
    "X_train_projected_vgg,X_model = get_lda(X_train_vgg,y_train_vgg,14)\n",
    "c = LogClassifier()\n",
    "c.train(X_train_projected_vgg,y_train_vgg)\n",
    "X_test_projected_vgg = X_model.transform(X_test_vgg)\n",
    "predicted,confusion,val_accuracy,precision,recall,f1 = c.validate(X_test_projected_vgg,y_test_vgg)\n",
    "\n",
    "lst.extend(['LDA of VGG',X_train_projected_vgg.shape[1],(1-val_accuracy)*100,val_accuracy*100,f1]) \n",
    "mat.append(lst)\n",
    "cfs.append(confusion)\n",
    "    \n",
    "# PCA of resnet\n",
    "lst = []\n",
    "X_train_projected_res,_,X_model = get_pca(X_train_res,n)\n",
    "c = LogClassifier()\n",
    "c.train(X_train_projected_res,y_train_res)\n",
    "X_test_projected_res = X_model.transform(X_test_res)\n",
    "predicted,confusion,val_accuracy,precision,recall,f1 = c.validate(X_test_projected_res,y_test_res)\n",
    "\n",
    "lst.extend(['PCA of resnet',X_train_projected_res.shape[1],(1-val_accuracy)*100,val_accuracy*100,f1]) \n",
    "mat.append(lst)\n",
    "cfs.append(confusion)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(mat)\n",
    "df.columns = ['Features/Combination of features','Reduced Dimensional Space','Classification error','Accuracy','F1 score']\n",
    "# Print the output.\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_transforms = [get_pca,\\\n",
    "    get_kernel_pca,\\\n",
    "    get_lda,\\\n",
    "    get_kernel_lda]  \n",
    "components = [n1,n1,10,10]\n",
    "titles = [\"PCA\",'Kernel PCA(poly=3)','LDA','Kernel LDA(poly=3)']\n",
    "\n",
    "mat = []\n",
    "cfs = []\n",
    "for i,ft in enumerate(feature_transforms):\n",
    "    print(i,ft)\n",
    "    lst = []\n",
    "    if i==0:#pca\n",
    "        X1_train_projected,_,X1_model = ft(X1_train,components[i])\n",
    "    elif i==1:#kpca\n",
    "        X1_train_projected,X1_model = ft(X1_train,components[i])\n",
    "    else:#lda and klda\n",
    "        X1_train_projected,X1_model = ft(X1_train,y1_train,components[i])\n",
    "        \n",
    "    c = LogClassifier()\n",
    "    c.train(X1_train_projected,y1_train)\n",
    "    X1_test_projected = X1_model.transform(X1_test)\n",
    "    predicted,confusion,val_accuracy,precision,recall,f1 = c.validate(X1_test_projected,y1_test)\n",
    "\n",
    "    lst.extend([titles[i],X1_train_projected.shape[1],(1-val_accuracy)*100,val_accuracy*100,f1]) \n",
    "    mat.append(lst)\n",
    "    cfs.append(confusion)\n",
    "\n",
    "features =  [X1_vgg,X1_res]\n",
    "titles = ['VGG','Resnet']\n",
    "for i,ft in enumerate(features):\n",
    "    lst = []\n",
    "    training_data, validation_data, training_labels, validation_labels = train_test_split(ft,y1, test_size = 0.2, random_state = 21)\n",
    "    c = LogClassifier()\n",
    "    c.train(training_data,training_labels)\n",
    "    predicted,confusion,val_accuracy,precision,recall,f1 = c.validate(validation_data,validation_labels)\n",
    "    lst.extend([titles[i],X1_train_projected.shape[1],(1-val_accuracy)*100,val_accuracy*100,f1])   \n",
    "    mat.append(lst)\n",
    "    cfs.append(confusion)\n",
    "\n",
    "# LDA of VGG\n",
    "lst = []\n",
    "X1_train_projected_vgg,X1_model = get_lda(X1_train_vgg,y1_train_vgg,14)\n",
    "c = LogClassifier()\n",
    "c.train(X1_train_projected_vgg,y1_train_vgg)\n",
    "X1_test_projected_vgg = X1_model.transform(X1_test_vgg)\n",
    "predicted,confusion,val_accuracy,precision,recall,f1 = c.validate(X1_test_projected_vgg,y1_test_vgg)\n",
    "\n",
    "lst.extend(['LDA of VGG',X1_train_projected_vgg.shape[1],(1-val_accuracy)*100,val_accuracy*100,f1]) \n",
    "mat.append(lst)\n",
    "cfs.append(confusion)\n",
    "    \n",
    "# PCA of resnet\n",
    "lst = []\n",
    "X1_train_projected_res,_,X1_model = get_pca(X1_train_res,n)\n",
    "c = LogClassifier()\n",
    "c.train(X1_train_projected_res,y1_train_res)\n",
    "X1_test_projected_res = X1_model.transform(X1_test_res)\n",
    "predicted,confusion,val_accuracy,precision,recall,f1 = c.validate(X1_test_projected_res,y1_test_res)\n",
    "\n",
    "lst.extend(['PCA of resnet',X1_train_projected_res.shape[1],(1-val_accuracy)*100,val_accuracy*100,f1]) \n",
    "mat.append(lst)\n",
    "cfs.append(confusion)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(mat)\n",
    "df.columns = ['Features/Combination of features','Reduced Dimensional Space','Classification error','Accuracy','F1 score']\n",
    "# Print the output.\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSNE on LDA and PCA plots\n",
    "ldas = [get_lda(X,y,10)[0],get_lda(X1,y1,6)[0]]\n",
    "ys = [y,y1]\n",
    "titles = [\"TSNE on Yale LDA\",\"TSNE on IMFDB LDA\"]\n",
    "for i,p in enumerate(ldas):\n",
    "    TSNE_features(p,ys[i],3,titles[i])\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X_pca[:,0],X_pca[:,1],X_pca[:,2],c=y)\n",
    "plt.title(\"Yale PCA\" )\n",
    "plt.show()\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X1_pca[:,0],X1_pca[:,1],X1_pca[:,2],c=y1)\n",
    "plt.title(\"IMFDB PCA\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = Isomap(n_components=3)\n",
    "X_i = embedding.fit_transform(X)\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X_i[:,0],X_i[:,1],X_i[:,2],c=y)\n",
    "plt.title(\"Yale Isomap\" )\n",
    "plt.show()\n",
    "embedding = Isomap(n_components=3)\n",
    "X_i = embedding.fit_transform(X1)\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X_i[:,0],X_i[:,1],X_i[:,2],c=y1)\n",
    "plt.title(\"IMFDB isomap\" )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold(Xp,yp,ks):\n",
    "    mat = []\n",
    "    p = ks.shape[0]\n",
    "    for nf in ks:\n",
    "        kf = KFold(n_splits=nf,shuffle=True)\n",
    "        kf.get_n_splits(Xp)\n",
    "        accuracies = []\n",
    "        precisions = []\n",
    "        for train_index, test_index in kf.split(Xp):\n",
    "            X_train, X_test = Xp[train_index], Xp[test_index]\n",
    "            y_train, y_test = yp[train_index], yp[test_index]\n",
    "\n",
    "            X_train_projected,X_model = get_lda(X_train,y_train,10)\n",
    "            c = LogClassifier()\n",
    "            c.train(X_train_projected,y_train)\n",
    "            \n",
    "            X_test_projected = X_model.transform(X_test)\n",
    "            _,_,val_accuracy,precision,_,_ = c.validate(X_test_projected,y_test)\n",
    "            accuracies.append(val_accuracy)\n",
    "            precisions.append(precision)\n",
    "        accuracies = np.array(accuracies)\n",
    "        precisions = np.array(precisions)\n",
    "        acc = np.mean(accuracies)\n",
    "        prec = np.mean(precisions)\n",
    "        mat.append([acc*100,prec])\n",
    "    matrix = np.array(mat)\n",
    "    ks =np.array(ks).reshape(p,1)\n",
    "    matrix = np.concatenate((ks,matrix),axis=1)    \n",
    "    return matrix\n",
    "\n",
    "ks = np.arange(3,24,3)\n",
    "print(\"For yale:\")\n",
    "mat = kfold(X,y,ks)\n",
    "df = pd.DataFrame(mat)\n",
    "df.columns = ['K splits','Accuracy','Precision']\n",
    "# Print the output.\n",
    "print(df.to_string(index=False))\n",
    "print(\"For IMFDB:\")\n",
    "mat = kfold(X1,y1,ks)\n",
    "df = pd.DataFrame(mat)\n",
    "df.columns = ['K splits','Accuracy','Precision']\n",
    "# Print the output.\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show quantitative results such as examples of correct prediction and wrong prediction\n",
    "X_train_projected,_,X_model = get_pca(X_train,65)\n",
    "c = LogClassifier()\n",
    "c.train(X_train_projected,y_train)\n",
    "X_test_projected = X_model.transform(X_test)\n",
    "predicted,_,acc,_,_,_ = c.validate(X_test_projected,y_test)\n",
    "mask = (predicted==y_test)\n",
    "nt = X_test.shape[0]\n",
    "X_3d = X_test.reshape((nt,H,W,C))\n",
    "ft = np.where(mask==True)[0][0]\n",
    "ff = np.where(mask==False)[0][0]\n",
    "plt.imshow(X_3d[ft])\n",
    "plt.title(\"Prediction :\"+str(le.inverse_transform([predicted[ft]])[0])+\" | Actual:\"+str(le.inverse_transform([y_test[ft]])[0]) )\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.imshow(X_3d[ff])\n",
    "plt.title(\"Prediction :\"+str(le.inverse_transform([predicted[ff]])[0])+\" | Actual:\"+str(le.inverse_transform([y_test[ff]])[0]) )\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show quantitative results such as examples of correct prediction and wrong prediction\n",
    "X1_train_projected,_,X1_model = get_pca(X1_train,65)\n",
    "c = LogClassifier()\n",
    "c.train(X1_train_projected,y1_train)\n",
    "X1_test_projected = X1_model.transform(X1_test)\n",
    "predicted,_,acc,_,_,_ = c.validate(X1_test_projected,y1_test)\n",
    "mask = (predicted==y1_test)\n",
    "nt = X1_test.shape[0]\n",
    "X1_3d = X1_test.reshape((nt,H,W,C))\n",
    "ft = np.where(mask==True)[0][0]\n",
    "ff = np.where(mask==False)[0][0]\n",
    "plt.imshow(X1_3d[ft])\n",
    "plt.title(\"Prediction :\"+str(le1.inverse_transform([predicted[ft]])[0])+\" | Actual:\"+str(le1.inverse_transform([y1_test[ft]])[0]) )\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.imshow(X1_3d[ff])\n",
    "plt.title(\"Prediction :\"+str(le1.inverse_transform([predicted[ff]])[0])+\" | Actual:\"+str(le1.inverse_transform([y1_test[ff]])[0]) )\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
